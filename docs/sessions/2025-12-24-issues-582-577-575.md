# Session: Issues #582, #577, #575

**Date:** 2025-12-24
**Issues:**

- #582: fix: eBay source URLs storing invalid item IDs
- #577: feat: Move publisher tier mappings to database
- #575: perf: Add pg_trgm index for publisher fuzzy matching

## Workflow

- PRs require user review before merging to staging
- PRs require user review before merging to production

---

## Issue #582: eBay source URLs storing invalid item IDs

### Problem

Some books have invalid eBay `source_url` and `source_item_id` values - short hex codes instead of proper 12-digit eBay item IDs.

**Examples:**

- Book 534: `c492afa0` (invalid)
- Book 537: `278a413f` (invalid) - FIXED manually
- Book 539: `238c5ab8` (invalid) - FIXED manually

### Root Cause (FOUND)

**File:** `scraper/handler.py` lines 37-45

```python
def extract_item_id(url: str) -> str:
    match = re.search(r"/itm/(\d+)", url)  # Only numeric
    if match:
        return match.group(1)
    match = re.search(r"item=(\d+)", url)
    if match:
        return match.group(1)
    return str(uuid.uuid4())[:8]  # BUG: Random UUID fallback!
```

**Data flow:**

1. User enters URL with alphanumeric short ID: `/itm/c492afa0`
2. API's `normalize_ebay_url()` correctly resolves via HTTP redirect
3. Scraper Lambda receives original URL, calls `extract_item_id()`
4. Regex `/itm/(\d+)` fails on alphanumeric ID
5. Random UUID[:8] returned and stored as `source_item_id`

**Fix approach:** Pass the resolved `item_id` to the scraper Lambda instead of letting it extract from URL.

### Status

- [x] Root cause investigation
- [x] Add backend fix (pass item_id to scraper)
- [x] Add scraper fix (use passed item_id)
- [x] Frontend fix (not needed - backend handles all validation)
- [x] Create PR for staging review - **PR #583**
- [x] **CODE REVIEW FIXES IMPLEMENTED** (see below)
- [x] **COMMIT AND PUSH** fixes to PR #583 (CI passed)
- [ ] Merge to staging after approval
- [ ] Create PR for production
- [ ] Merge to production after approval

### Changes Made (Initial)

1. **backend/app/api/v1/listings.py**
   - Pass resolved `item_id` to scraper Lambda in async payload
   - Pass `item_id` to `scrape_ebay_listing()` in sync endpoint
   - Use resolved `item_id` in response (not scraper's fallback)

2. **backend/app/services/scraper.py**
   - Add `item_id` parameter to `invoke_scraper()` and `scrape_ebay_listing()`
   - Pass `item_id` in Lambda payload

3. **scraper/handler.py**
   - Update `extract_item_id()` to accept `provided_id` parameter
   - Use provided ID when available (handles alphanumeric short IDs)
   - Log warning when falling back to random UUID

### Code Review Feedback (MUST ADDRESS)

**PR #583 received code review with these issues:**

#### 1. CRITICAL: Zero Test Coverage

No automated tests for the bug fix. Required tests:

- `test_extract_item_id_uses_provided_id_when_given()`
- `test_extract_item_id_logs_warning_on_fallback()`
- `test_invoke_scraper_passes_item_id_in_payload()`
- `test_extract_listing_async_passes_item_id()`

#### 2. HIGH: Silent Empty String on Missing Item ID

```python
ebay_item_id=item_id or result.get("item_id", "")
```

Edge cases:

- What if both are None? Returns empty string - may cause downstream issues
- What if item_id is "0" or falsy-looking valid ID?

**Fix needed:** Raise exception instead of returning empty string

#### 3. HIGH: UUID Fallback Still Generates Junk Data

```python
return str(uuid.uuid4())[:8]  # Still generates garbage
```

"Should rarely happen" is not acceptable. Should raise exception, not silently corrupt database.

**Fix needed:** Raise ValueError instead of returning random UUID

#### 4. HIGH: IntegrityError Assumption in Race Condition Fix (FIXED)

**File:** `backend/app/services/publisher_validation.py`

The `except IntegrityError:` block assumed all IntegrityErrors were duplicate name constraint violations.

**Fix applied:**

```python
except IntegrityError as e:
    error_str = str(e.orig) if e.orig else str(e)
    if "publishers_name_key" not in error_str and "UNIQUE constraint" not in error_str:
        raise  # Re-raise non-name constraint errors
    # ... handle duplicate name case ...
    raise RuntimeError(...) from e  # Preserve traceback
```

### Code Review Fixes IMPLEMENTED

All three code review issues have been addressed:

#### Fix 1: Tests Added (DONE)

**Files:**

- `scraper/test_handler.py` - Added `TestExtractItemId` class with 9 tests
- `backend/tests/test_scraper_service.py` - Added `TestInvokeScraperItemIdParameter` class and 2 new tests to `TestScrapeEbayListing`

**Tests added:**

- `test_uses_provided_id_when_given()`
- `test_uses_provided_id_for_alphanumeric_short_urls()`
- `test_extracts_from_standard_url()`
- `test_extracts_from_item_param_url()`
- `test_raises_on_alphanumeric_url_without_provided_id()`
- `test_raises_on_invalid_url()`
- `test_raises_on_empty_url()`
- `test_provided_id_empty_string_not_used()`
- `test_provided_id_none_extracts_from_url()`
- `test_passes_item_id_in_payload()`
- `test_omits_item_id_when_not_provided()`
- `test_uses_passed_item_id_over_scraper_result()`
- `test_raises_when_no_valid_item_id()`

All 15 backend tests PASS.

#### Fix 2: Empty String Edge Case (DONE)

**File:** `backend/app/api/v1/listings.py` lines 172-178

Changed:

```python
ebay_item_id=item_id or result.get("item_id", "")
```

To:

```python
final_item_id = item_id or result.get("item_id")
if not final_item_id:
    raise HTTPException(
        status_code=422,
        detail="Could not determine eBay item ID from URL or scraper result",
    )
ebay_item_id=final_item_id
```

**Also fixed:** `backend/app/services/scraper.py` lines 170-174

```python
final_item_id = item_id or scraper_result.get("item_id")
if not final_item_id:
    raise ValueError("Scraper did not return a valid item ID")
```

#### Fix 3: UUID Fallback Raises Exception (DONE)

**File:** `scraper/handler.py` lines 60-61

Changed:

```python
logger.warning(f"Could not extract numeric item ID from URL: {url}")
return str(uuid.uuid4())[:8]
```

To:

```python
raise ValueError(f"Could not extract eBay item ID from URL: {url}")
```

Also removed unused `import uuid`.

### Next Steps for PR #583

All code review fixes have been committed and CI has passed.

1. ~~COMMIT the fixes~~ ✅ DONE
2. ~~PUSH to branch `fix/ebay-short-id-validation`~~ ✅ DONE
3. ~~Wait for CI to pass~~ ✅ DONE

**Current Status: AWAITING USER REVIEW**

1. **User reviews PR #583** → merge to staging
2. **Validate in staging environment**
3. **Create PR from staging to main** → production
4. **User reviews production PR** → merge to main

---

## Issue #577: Move publisher tier mappings to database

### Problem

`TIER_1_PUBLISHERS` and `TIER_2_PUBLISHERS` are hardcoded Python dictionaries.

### Priority

Low - publisher additions are rare (~1/month)

### Status

- [ ] Design approach
- [ ] Implementation
- [ ] PR for staging review
- [ ] PR for production

---

## Issue #575: Add pg_trgm index for publisher fuzzy matching

### Problem

`fuzzy_match_publisher()` loads ALL publishers into memory - O(n) table scan.

### Priority

Low - current scale (~50 publishers) has negligible impact.

### Status

- [ ] Design approach
- [ ] Implementation
- [ ] PR for staging review
- [ ] PR for production

---

## Progress Log

### 2025-12-24 (Session 4 - Additional Code Review Fix)

- Addressed additional code review: IntegrityError handling in publisher_validation.py
- **Issue**: `except IntegrityError:` assumed all IntegrityErrors were duplicate name violations
- **Fix**: Check for `publishers_name_key` (PostgreSQL) or `UNIQUE constraint` (SQLite) before handling
- **Fix**: Changed `from None` to `from e` to preserve traceback
- Committed: `fix: Handle only name unique constraint in IntegrityError`
- Pushed to branch `fix/ebay-short-id-validation`
- **CI PASSED** - All checks green
- All 42 publisher_validation tests PASS locally
- **PR #583 READY FOR USER REVIEW** → merge to staging

### 2025-12-24 (Session 3 - Push and CI)

- Committed and pushed code review fixes to PR #583
- Fixed formatting issue in test_scraper_service.py
- **ALL CI CHECKS PASSED**
- PR #583 ready for user review → merge to staging

### 2025-12-24 (Session 2 - Code Review Fixes)

- Resumed session to address PR #583 code review feedback
- Using **receiving-code-review** skill
- **Fix 1 DONE**: Added 13 automated tests across 2 test files
- **Fix 2 DONE**: Fixed empty string edge case - now raises HTTPException/ValueError
- **Fix 3 DONE**: Fixed UUID fallback - now raises ValueError instead of generating garbage
- All 15 backend tests PASS
- Linting PASS

### 2025-12-24 (Session 1 - Initial Investigation)

- Session started
- Issues reviewed
- **Issue #582 root cause found**: scraper's `extract_item_id()` falls back to random UUID for alphanumeric short IDs
- **PR #583 created** targeting staging branch
- **Code review feedback received** - must address before merge:
  - Add automated tests (CRITICAL)
  - Fix empty string edge case (HIGH)
  - Raise exception instead of UUID fallback (HIGH)

---

## MANDATORY: Superpowers Skills Usage

**CRITICAL: Always use Superpowers skills at ALL stages of work.**

### Required Skills by Task Type

| Task Type | Skills to Use (in order) |
|-----------|-------------------------|
| Debugging | `systematic-debugging` → `root-cause-tracing` → `defense-in-depth` |
| New features | `brainstorming` → `using-git-worktrees` → `writing-plans` → `subagent-driven-development` |
| Writing tests | `test-driven-development` → `condition-based-waiting` → `testing-anti-patterns` |
| Code review | `requesting-code-review` → `receiving-code-review` |
| Completing work | `verification-before-completion` → `finishing-a-development-branch` |

### Key Reminders

1. **NEVER skip skills** - even for "simple" tasks
2. **Use TodoWrite** for all checklists in skills
3. **Announce skill usage** before using
4. **Follow chains completely** - don't cherry-pick

### Bash Command Rules (NEVER BREAK THESE)

**NEVER use:**

- `#` comment lines before commands
- `\` backslash line continuations
- `$(...)` command substitution
- `||` or `&&` chaining
- `!` in quoted strings

**ALWAYS use:**

- Simple single-line commands
- Separate sequential Bash tool calls
- `bmx-api` for all BlueMoxon API calls
