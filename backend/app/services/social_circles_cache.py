# backend/app/services/social_circles_cache.py

"""Redis caching for social circles graph data.

Provides caching with configurable TTL and graceful degradation when Redis
is unavailable. Uses the existing Redis client from app.cache.

Cache key format: social_circles:graph:{hash_of_params}
Response headers: X-Cache: HIT|MISS
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import logging
from typing import TYPE_CHECKING

from app.cache import get_redis
from app.schemas.social_circles import SocialCirclesResponse

if TYPE_CHECKING:
    from redis import Redis

logger = logging.getLogger(__name__)

# Cache TTL in seconds (5 minutes)
CACHE_TTL_SECONDS = 300

# Cache key prefix for social circles graph data
CACHE_KEY_PREFIX = "social_circles:graph"

# Maximum cache entry size in bytes (10 MB)
# Prevents caching extremely large graphs that could cause memory issues
MAX_CACHE_SIZE_BYTES = 10 * 1024 * 1024


def get_cache_key(include_binders: bool, min_book_count: int, max_books: int) -> str:
    """Generate deterministic cache key from query parameters.

    Creates a hash of the parameters to ensure consistent key generation
    regardless of parameter order.

    Args:
        include_binders: Whether binder nodes are included.
        min_book_count: Minimum book count filter.
        max_books: Maximum books limit.

    Returns:
        Cache key string in format: social_circles:graph:{hash}
    """
    # Create a stable string representation of parameters
    params_str = f"binders={include_binders}|min={min_book_count}|max={max_books}"
    params_hash = hashlib.sha256(params_str.encode()).hexdigest()[:16]
    return f"{CACHE_KEY_PREFIX}:{params_hash}"


def _get_cached_graph_sync(
    client: Redis, cache_key: str
) -> tuple[SocialCirclesResponse | None, bool]:
    """Synchronous implementation of cache get.

    Args:
        client: Redis client instance.
        cache_key: Cache key to look up.

    Returns:
        Tuple of (response or None, is_cache_hit).
    """
    try:
        cached_value = client.get(cache_key)
        if cached_value:
            logger.debug("Cache HIT: %s", cache_key)
            data = json.loads(cached_value)
            return SocialCirclesResponse.model_validate(data), True
        logger.debug("Cache MISS: %s", cache_key)
        return None, False
    except Exception as e:
        logger.warning("Redis GET failed for %s: %s", cache_key, e)
        return None, False


async def get_cached_graph(cache_key: str) -> tuple[SocialCirclesResponse | None, bool]:
    """Retrieve cached graph if available.

    Runs Redis GET operation in executor to avoid blocking the async event loop.

    Args:
        cache_key: Cache key generated by get_cache_key().

    Returns:
        Tuple of (SocialCirclesResponse or None, is_cache_hit).
        The boolean indicates whether this was a cache hit (True) or miss (False).
        If Redis is unavailable, returns (None, False).
    """
    client = get_redis()
    if not client:
        logger.debug("Cache MISS (no Redis): %s", cache_key)
        return None, False

    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, _get_cached_graph_sync, client, cache_key)


def _set_cached_graph_sync(client: Redis, cache_key: str, response: SocialCirclesResponse) -> None:
    """Synchronous implementation of cache set.

    Args:
        client: Redis client instance.
        cache_key: Cache key to store under.
        response: Response data to cache.
    """
    try:
        # Serialize using Pydantic's model_dump with JSON-compatible output
        serialized = json.dumps(response.model_dump(mode="json"), default=str)

        # Skip caching if serialized data exceeds size limit
        if len(serialized) > MAX_CACHE_SIZE_BYTES:
            logger.warning(
                "Skip caching %s: size %d bytes exceeds limit %d bytes",
                cache_key,
                len(serialized),
                MAX_CACHE_SIZE_BYTES,
            )
            return

        client.setex(cache_key, CACHE_TTL_SECONDS, serialized)
        logger.debug("Cached %s with TTL %ds", cache_key, CACHE_TTL_SECONDS)
    except Exception as e:
        logger.warning("Redis SETEX failed for %s: %s", cache_key, e)


async def set_cached_graph(cache_key: str, response: SocialCirclesResponse) -> None:
    """Cache graph response with TTL.

    Runs Redis SETEX operation in executor to avoid blocking the async event loop.
    Fails silently if Redis is unavailable - caching is optional.

    Args:
        cache_key: Cache key generated by get_cache_key().
        response: SocialCirclesResponse to cache.
    """
    client = get_redis()
    if not client:
        logger.debug("Cache skip (no Redis): %s", cache_key)
        return

    loop = asyncio.get_running_loop()
    await loop.run_in_executor(None, _set_cached_graph_sync, client, cache_key, response)


def _invalidate_cache_sync(client: Redis) -> int:
    """Synchronous implementation of cache invalidation.

    Note: This uses SCAN + DELETE which is not atomic. In high-concurrency
    scenarios, keys could be added between scan and delete, leaving stale data.
    This is acceptable because:
    1. Cache entries have a TTL, so stale data will expire naturally
    2. Full invalidation is rare (only on data changes)
    3. Atomicity is not critical for cache - stale reads are tolerable

    Args:
        client: Redis client instance.

    Returns:
        Number of keys deleted.
    """
    try:
        # Find all social circles cache keys using SCAN (cursor-based, safe for large keyspaces)
        pattern = f"{CACHE_KEY_PREFIX}:*"
        keys = list(client.scan_iter(match=pattern))
        if keys:
            deleted = client.delete(*keys)
            logger.info("Invalidated %d social circles cache entries", deleted)
            return deleted
        logger.debug("No social circles cache entries to invalidate")
        return 0
    except Exception as e:
        logger.warning("Redis cache invalidation failed: %s", e)
        return 0


def invalidate_cache() -> int:
    """Invalidate all social circles cache entries.

    This is a synchronous function for use in non-async contexts (e.g., book updates).
    Uses SCAN to find matching keys and DELETE to remove them.

    Returns:
        Number of cache entries invalidated. Returns 0 if Redis unavailable.
    """
    client = get_redis()
    if not client:
        logger.debug("Cache invalidation skip (no Redis)")
        return 0

    return _invalidate_cache_sync(client)


async def invalidate_cache_async() -> int:
    """Invalidate all social circles cache entries (async version).

    Runs cache invalidation in executor to avoid blocking the async event loop.

    Returns:
        Number of cache entries invalidated. Returns 0 if Redis unavailable.
    """
    client = get_redis()
    if not client:
        logger.debug("Cache invalidation skip (no Redis)")
        return 0

    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, _invalidate_cache_sync, client)
